# -*- coding: utf-8 -*-
"""Decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A-bpdq_m3DudBf23LrzTEdsNSDgBH10U

Loading Data and split
"""

import pandas as pd
import numpy as np
import math
import copy
import pprint
import matplotlib.pyplot as plt
import sys
from sklearn.model_selection import train_test_split

depth = 4
if len(sys.argv) == 2:
  depth = int(sys.argv[1])

def preprocess(df):
  df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)
  df['Date'] = df['Date'].dt.date

  #taking chunks of 10 days each
  for i in range(len(df)): 
    date = df.iloc[i, 1]
    day = date.day
    if day >= 1 and day <= 10:
      day = 5
    elif day >= 11 and day <= 20:
      day = 15
    else:
      day = 25
    date = date.replace(day = day)
    df.iloc[i, 1] = date


  for i in range(len(df)):
    df.iloc[i, 4] = (df.iloc[i, 4] + 4)//5;
    df.iloc[i, 5] = (df.iloc[i, 5] + 4)//5;
    df.iloc[i, 6] = (df.iloc[i, 6] + 4)//5;
    df.iloc[i, 8] = (df.iloc[i, 8] + 4)//5;
  return df

"""Count the number of target_variable that we can get using attribute value"""

def counter(data,att,variable,target_variable):
	den = 0
	num = 0
	for index,val in data[att].items():
		if(val == variable):
			den+=1
			if(data['Deaths'][index] == target_variable):
				num+=1
	return num,den

"""Used to calculate entropy"""

def attribute_entropy(data,attribute):
	y = np.unique(np.array(data['Deaths'])) 
	variables = np.unique(np.array(data[attribute])) 
	total = 0
	non_zero = np.finfo(float).eps
	for variable in variables:
		entropy = 0
		for target_variable in y:
			num,den = counter(data,attribute,variable,target_variable)
			fraction = (num)/(den+non_zero)
			entropy += -fraction*np.log2(fraction)
		fraction2 = den/(len(data)+non_zero)
		total += -fraction2*entropy
	return abs(total)

"""After selecting node with maximum value used to divide the testcases"""

def partition(data,node,value):
	new = {}
	for i in (list(data.keys())):
		new[i] = []
	output = 'Deaths'
	for index,val in data[node].items():
		if(val==value):
				for i in (list(data.keys())):
						new[i].append(data[i][index])
	del new[node]
	new=pd.DataFrame(new)		
	return new

"""Decision Tree Function"""

def Tree(data, attributes,depth): 
	output = 'Deaths'

	#Calculating Entropy of dataset
	node_entropy = 0
	all_ = np.array(data[output])
	unique, counts = np.unique(all_, return_counts=True)
	for i in range(len(unique)):
		fraction = counts[i]/len(data[output])
		node_entropy += -fraction*np.log2(fraction)

	if(len(attributes)==0 or depth==1):
		clValue,counts = np.unique( np.array( data[output] ),return_counts=True)	
		_class = clValue[np.argmax(counts)]
		out_list=[]
		out_list.append(output)
		out_list.append(_class)
		return out_list

  #Finding attribute with maximum gain
	gain=0
	node=attributes[0]
	for key in attributes:
		curr_gain=(node_entropy)-(attribute_entropy(data,key))
		if(curr_gain>gain):
			 gain=curr_gain
			 node=key
	vals = np.unique(np.array(data[node]))      
	attributes_new= attributes.copy()
	attributes_new.remove(node)            
	tree=[]
	valuetree = {}
	for value in vals:
			new_data=partition(data,node,value)
			valuetree[value]=Tree(new_data,attributes_new,depth-1)
	tree.append(node)
	tree.append(valuetree)				   
	return tree

"""Accuracy Function"""

def accuracy(tree,testSet):
  correct=0
  total=0
  total= len(testSet)
  for index,test in testSet.iterrows():
    checktree=tree
    while(1):
      if(checktree[0]=='Deaths'):
        if(checktree[1]==test['Deaths']):
          correct=correct+1
        break
      newdict=checktree[1]
      if test[checktree[0]] in newdict:
        checktree=newdict[test[checktree[0]]]
      else:
        break
  acc=correct/total
  return acc

"""Calculating accuracy of 10 random 80/20 training/test split with maximum depth specified"""

def accuracy_with_split(depth, df):
  average=0
  max_accuracy = 0
  attributes = ['Date','State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Confirmed','Cured']
  for i in range(10):
    data, testSet = train_test_split(df, test_size=0.2)
    tree = Tree(data,attributes,depth)
    acc = accuracy(tree,testSet)
    average += acc
    if acc > max_accuracy :
      max_accuracy = acc
      reqd_tree = tree
    
  average=average/10
  return average, reqd_tree

"""Accuracy v/s Depth"""

def best_depth_limit_plot(df):
  xaxis=np.arange(2,10,1)
  average=np.zeros(8)
  for i in range(2,10):
    average[i-2], tree = accuracy_with_split(i, df)
  plt.figure(figsize=(8, 6))
  plt.plot(xaxis,average)
  plt.show()

"""Function to plot the decision tree"""

def plot(tree,deepin=0):
  prefix = '|   '
  for key,values in tree[1].items():
    print(prefix * deepin + tree[0]+  '=' + str(key))
    if (values[0]!='Deaths'):
        plot(values, deepin=deepin + 1)
    else:
        print(prefix * (deepin+1)+values[0]+ '='+str(values[1]))

"""Function to get the majority vote for pruning"""

def get_counts(tree, depth, counts):
  if depth == 3:
    return
  for key, values in tree[1].items():
    if values[0] == 'Deaths':
      # print(values[1])
      counts[values[1]] += 1
    else:
      get_counts(values, depth + 1, counts)

"""Pruning"""

def post_prune(tree, depth, validation_set):
  d_best = copy.deepcopy(tree)

  for i in range(3): #pruning the attribute at depth i; date at depth 0, states at depth 1, indian nationals at depth 2
    d_prime = copy.deepcopy(tree)
    for key, val in d_prime[1].items():
      d_new = copy.deepcopy(d_prime)
      counts = np.zeros((6,), dtype=int)
      get_counts(val, i, counts)
      mx = np.argmax(counts)
      nw = ['Deaths', mx]
      d_new[1][key] = nw    
      
      if accuracy(d_new, validation_set) >= accuracy(d_prime, validation_set):
        d_prime = copy.deepcopy(d_new)

    if accuracy(d_prime, validation_set) >= accuracy(d_best, validation_set):
      d_best = copy.deepcopy(d_prime)

  return d_best

d = pd.read_csv('https://raw.githubusercontent.com/raghavpoddar-kgp/CS60050---Machine-Learning/master/IndiaCOVIDStatistics.csv')
df = pd.DataFrame(data = d)
df = preprocess(df)
data, testSet = train_test_split(df, test_size=0.2)
average_accuracy, tree = accuracy_with_split(depth, df)
print()
print()
print("Average accuracy over 10 random splits: " + str(average_accuracy))
print()
print("------------------------------------------------------------------")
print()
print("Plot of accuracy vs depth: ")
best_depth_limit_plot(df)
print()

print("------------------------------------------------------------------")
print()

train, validation_set =  train_test_split(data, test_size=0.2)
new_tree = post_prune(tree, 4, validation_set)
new_accuracy = accuracy(new_tree, validation_set)
print("Accuracy of pruned tree: " + str(new_accuracy))
print()
print("------------------------------------------------------------------")
print()
print("Final pruned tree: ")
print()
plot(new_tree)
